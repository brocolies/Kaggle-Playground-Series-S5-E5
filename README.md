# ğŸ§  Predict Calories from Biometric Data

> A regression challenge to predict calories burned using biometric and activity records.  
> Participated as part of Kaggle Playground Series S5E5.

---

## ğŸ” Problem Overview

- **Task:** Predict total calories consumed
- **Type:** Supervised regression
- **Metric:** RMSLE (Root Mean Squared Log Error)
- **Dataset:** 750k training / 250k test samples

---

## ğŸ§‘â€ğŸ’» What I Learned

This project was not just about modeling â€” it was about structuring my ML thinking. I learned:

1. **Data Distribution Matters**  
   â†’ Skewed target distributions can distort predictions; log-transforming helped align the loss function.

2. **Model Choice vs. Metric**  
   â†’ RMSLE penalizes under-predictions more â€” an insight that influenced feature scaling and outlier treatment.

3. **Feature Engineering is King**  
   â†’ Creating features like `EnergyProxy` (Intensity Ã— Duration) had measurable impact.

4. **Practical XGBoost Usage**  
   â†’ Early stopping, `eval_set`, and using validation fold properly allowed stable training.

5. **Organizing a Kaggle Pipeline**  
   â†’ End-to-end flow from EDA â†’ feature engineering â†’ validation â†’ inference â†’ submission is now repeatable.

---

## ğŸ§ª Experiment Summary

| Model         | CV Method        | RMSLE (val) | Notes                             |
|---------------|------------------|--------------|------------------------------------|
| XGBoostRegressor | train/test split (80:20) | 0.0629       | baseline model                     |
| Ridge          | train/test split | 0.074        | poor fit due to linear assumption |
| XGBoost tuned  | 5-Fold CV        | 0.058 (CV)   | better generalization              |

---

## ğŸ”¬ Key Features

| Feature       | Description                              |
|---------------|------------------------------------------|
| `BMI`         | Weight / HeightÂ²                         |
| `Intensity`   | Proxy for activity strength (HR + Temp)  |
| `EnergyProxy` | Duration Ã— Intensity                     |

---

## ğŸš€ Room for Improvement

- Refine feature selection by importance or SHAP values
- Switch to full K-Fold validation pipeline
- Experiment with ensembling (Voting / Stacking)
- Better handling of outliers & target clipping

---

## ğŸ“ Notes for Future Me

- Start from data distribution understanding, not from modeling.
- Never skip log-transform checks for regression metrics like RMSLE.
- Think: "If I had to redo this in 1 hour, what could I reuse or automate?"

---

## ğŸ”— Competition Link

[ğŸ‘‰ View on Kaggle](https://www.kaggle.com/competitions/your-competition)
